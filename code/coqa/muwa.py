# -*- coding: utf-8 -*-
"""notebookfbfdb0d280

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebookfbfdb0d280-ac36569f-b264-4fee-bcde-f7652967c19f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241120/auto/storage/goog4_request%26X-Goog-Date%3D20241120T194040Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1d15da6e2d2b701be67fb11fa8c085d1012b3d132c3d8eb332f25ea68f89391f432a1bb88916ec9a25845adf64fce7bdc13c0da4c60b71aab3c0401975542023b9e83f935a8ce1853126cb64542bd63e8dc1180900a10814fae0799e073b3bd01cce8052d482f2961c69f8982d61bcaa20a5c41a0faf05662bb3a69867aa6018748e972610ae5849a12cebc7b1ac9ff68252dde5e66a803fa2f3e9a45fa0f7f121a72af3012fe005b29ae91d779bde485ff8a3ce7c788d9dbfef074a61621909811d462c45f1a5ac2f14261ba9ccab4fe5a936f0ce5eb50c9a9ee55c8ab6749be1c3f50deddbb35185bbfa19184f49d9f090c3f571f70c4c7dc2071d5cabd766
"""

# Install necessary libraries
!pip install torch peft datasets evaluate transformers accelerate bitsandbytes selfcheckgpt
import torch
import torch.nn.functional as F
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
import os
import json
import math
from selfcheckgpt.modeling_selfcheck import SelfCheckBERTScore
import numpy as np

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Define model names
peft_model_id = 'theSLWayne/Muwa-1.3b'
base_model = 'facebook/opt-1.3b'
nli_model_name = 'roberta-large-mnli'

# Load models
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map='auto',
    torch_dtype=torch.float16,
)

qa_model = PeftModel.from_pretrained(
    model,
    peft_model_id,
    device_map='auto',
    torch_dtype=torch.float16,
)

nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device)
nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)
qa_tokenizer = AutoTokenizer.from_pretrained(base_model)

# Initialize SelfCheckBERT
selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)

# Load dataset
ds = load_dataset("stanfordnlp/coqa")
validation_data = ds['validation']

def check_entailment(premise, hypothesis):
    inputs = nli_tokenizer(premise, hypothesis, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = nli_model(**inputs)
    logits = outputs.logits
    entail_contradiction_logits = logits[:, [0, 2]]
    probs = F.softmax(entail_contradiction_logits, dim=1)
    entail_prob = probs[:, 0].item()
    return entail_prob

# Ensure clustering uses answer texts
def cluster_answers(answers, question, threshold):
    answer_texts = [answer["answer"] for answer in answers]
    clusters = []
    for answer_text in answer_texts:
        added_to_cluster = False
        for cluster in clusters:
            representative_answer = cluster[0]
            forward_entail_prob = check_entailment(question + " " + representative_answer, answer_text)
            backward_entail_prob = check_entailment(question + " " + answer_text, representative_answer)
            if forward_entail_prob > threshold and backward_entail_prob > threshold:
                cluster.append(answer_text)
                added_to_cluster = True
                break
        if not added_to_cluster:
            clusters.append([answer_text])
    return clusters

def calculate_cluster_probabilities(clusters, total_answers):
    return [len(cluster) / total_answers for cluster in clusters]

def calculate_semantic_entropy(cluster_probabilities):
    entropy = 0
    for prob in cluster_probabilities:
        if prob > 0:
            entropy -= prob * math.log2(prob)
    return entropy

def generate_answers_with_confidence(question, context, temperature, num_responses=20, max_new_tokens=20):
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)
    responses_with_confidence = []

    for _ in range(num_responses):
        with torch.no_grad():
            outputs = qa_model(input_ids=inputs.input_ids)
            logits = outputs.logits

            output_tokens = qa_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                top_k=50,
                temperature=temperature,
                return_dict_in_generate=True,
                output_scores=True,
            )

            scores = torch.stack(output_tokens.scores, dim=1)
            probs = F.softmax(scores, dim=-1)

            generated_tokens = output_tokens.sequences[0][inputs.input_ids.shape[1]:]
            token_probs = []

            for token_idx, token in enumerate(generated_tokens):
                if token_idx < len(output_tokens.scores):
                    token_prob = probs[0][token_idx][token]
                    token_probs.append(token_prob.item())

            confidence_score = sum(token_probs) / len(token_probs) if token_probs else 0
            generated_answer = qa_tokenizer.decode(generated_tokens, skip_special_tokens=True)

            # Add SelfCheckBERT score
            selfcheck_score = selfcheck_bertscore.predict(
                sentences=[generated_answer],
                sampled_passages=[context],
            )[0]

            responses_with_confidence.append({
                "answer": generated_answer,
                "confidence": confidence_score,
                "selfcheck_score": selfcheck_score,
                "correct": selfcheck_score > 0.7  # Using 0.7 as threshold
            })

    responses_with_confidence.sort(key=lambda x: x["confidence"], reverse=True)
    return responses_with_confidence

# Directory to store results
output_dir = "qa_analysis_results"
os.makedirs(output_dir, exist_ok=True)

# Parameters
temperatures = 0.5
num_responses = 20
response_counts = 20
entailment_thresholds = 0.3
selfcheck_thresholds = 0.5

# Store all results
all_results = []

# Process first example only for testing
max_examples = 1
x = 0

all_results = []

# Processing results
for temperature in temperatures:
    for idx, example in enumerate(validation_data):
        if idx >= max_examples:
            break

        context = example['story']
        questions = example['questions']
        answers_list = [{"answer": ans} for ans in example['answers']['input_text']]  # Convert to list of dicts

        for n in range(len(questions)):
            question = questions[n]
            reference_answers = [answers_list[n]]

            # Generate responses for the current question
            responses = generate_answers_with_confidence(question, context, temperature)

            for count in response_counts:
                subset_responses = responses[:min(count, len(responses))]

                for selfcheck_threshold in selfcheck_thresholds:
                    classified_responses = [
                        {
                            "answer": ans["answer"],
                            "confidence": ans["confidence"],
                            "correct": ans["selfcheck_score"] > selfcheck_threshold,
                        }
                        for ans in subset_responses
                    ]

                    for entailment_threshold in entailment_thresholds:
                        clusters = cluster_answers(subset_responses, question, entailment_threshold)
                        cluster_probabilities = [len(cluster) / len(classified_responses) for cluster in clusters]
                        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)

                        result = {
                            "temperature": temperature,
                            "response_count": count,
                            "selfcheck_threshold": selfcheck_threshold,
                            "entailment_threshold": entailment_threshold,
                            "question": question,
                            "responses": classified_responses,
                            "clusters": clusters,  # No indexing needed
                            "semantic_entropy": semantic_entropy,
                        }
                        all_results.append(result)

# Function to convert non-serializable objects to serializable ones
def make_serializable(obj):
    if isinstance(obj, (np.bool_, np.ndarray)):  # Handle NumPy-specific types
        return obj.tolist()  # Convert to Python-native types
    elif isinstance(obj, torch.Tensor):  # Handle PyTorch tensors
        return obj.tolist()  # Convert tensor to list
    elif isinstance(obj, bool):  # Ensure native Python bool
        return bool(obj)
    elif isinstance(obj, (np.float32, np.float64)):  # Handle NumPy floats
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64)):  # Handle NumPy integers
        return int(obj)
    else:
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

# Update result serialization step
output_file = "all_results.json"
serializable_results = []

for result in all_results:
    try:
        # Recursively apply make_serializable to the result dictionary
        serializable_result = json.loads(
            json.dumps(result, default=make_serializable)
        )
        serializable_results.append(serializable_result)
    except TypeError as e:
        print(f"Skipping a result due to serialization error: {e}")

# Save results
with open(output_file, "w") as f:
    json.dump(serializable_results, f, indent=4)

print(f"All results saved to {output_file}.")