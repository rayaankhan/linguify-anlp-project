# -*- coding: utf-8 -*-
"""roberta-coqa-final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/roberta-coqa-final-52a5e4d5-cdab-4b7d-abd4-c5eedefe813b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241120/auto/storage/goog4_request%26X-Goog-Date%3D20241120T194437Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D03ff3771207c569a70e59f761efe4175ad1287dbe7f720304ec61e8d3f7febee8cd1c020ed07704b78bf5737e66da51d058fc29b953e8362f9ed7ab3349fb06d1e189cffddd212e2e05feca38630a9bf57ee7f8b0211d4c93a4fe4fb7ff58459edb38e0180168d6aa25bc4cf801faf025ea10f5d2f835d32746f1f0dc7c638b6329b961efdcc8199fb714b4c9aa4c22f60423acf6fe4b1c35fbdeda3e9dbe89b8b4003ea7d108d76fd68a04dff7754331b9a9b63810d5fa4c9aa866b88c1bce161004554331abcaa6d45c98d1fa057006a087634b23ad5bf38c05b5c2708b38fbb4adfae075ccc914e69144bf13212fae386953128d2136d5783a0716933517e
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

rayaankhan696969_trial_ds_path = kagglehub.dataset_download('rayaankhan696969/trial-ds')
rayaankhan696969_train_ds_2_path = kagglehub.dataset_download('rayaankhan696969/train-ds-2')

print('Data source import complete.')

print("1")

!pip install datasets
!pip install rouge_score
!pip install accelerate
!pip install selfcheckgpt
!pip install nltk
!pip install evaluate
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F
from rouge_score import rouge_scorer
import json
import math
from sklearn.metrics import roc_auc_score
import numpy as np
import spacy
from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram, SelfCheckNLI, SelfCheckLLMPrompt
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import evaluate

print("JAI MATA DI!")
# input_file_path = '/kaggle/input/train-ds-2/train-v2.0.json'
# with open(input_file_path, 'r') as f:
#   squad_data = json.load(f)

# squad_examples = squad_data['data']

rouge_metric = evaluate.load("rouge")
qa_model_name = "deepset/roberta-base-squad2"
qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)
qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)

nli_model_name = "facebook/bart-large-mnli"
nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)
nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)

clm_model_name = "gpt2"
clm_model = AutoModelForCausalLM.from_pretrained(clm_model_name)
clm_tokenizer = AutoTokenizer.from_pretrained(clm_model_name)


qa_model.to(device)
nli_model.to(device)
clm_model.to(device)

from datasets import load_dataset

ds = load_dataset("stanfordnlp/coqa")
train_data = ds['validation']

def generate_answers(question, context, top_n=5):
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = qa_model(**inputs)
    start_probs = F.softmax(outputs.start_logits, dim=1).squeeze(0)
    end_probs = F.softmax(outputs.end_logits, dim=1).squeeze(0)
    max_answer_length = 30
    answer_spans = []
    for start_idx, start_prob in enumerate(start_probs):
        for end_idx, end_prob in enumerate(end_probs[start_idx:start_idx + max_answer_length]):
            score = start_prob * end_prob
            answer_spans.append((start_idx, start_idx + end_idx, score))
    answer_spans = sorted(answer_spans, key=lambda x: x[2], reverse=True)[:top_n]
    answers = []
    for start_idx, end_idx, score in answer_spans:
        answer_tokens = inputs["input_ids"][0][start_idx:end_idx + 1]
        answer = qa_tokenizer.decode(answer_tokens)
        answers.append((answer, score.item()))
    return answers


# CHECKING THE SEMANTIC SIMILARITY USING THE BIDIRECTIONAL ENTAILMENT ALGORITHM.RESEARCH PAPER:https://arxiv.org/pdf/1911.00681.pdf
# Bi-directional entailment involves checking whether two texts (typically a hypothesis and a premise) can entail each other, implying a deep semantic similarity or paraphrase relationship.
# Mathematical Basis of Bi-directional Entailment:
# To implement bi-directional entailment, each text is considered both as a hypothesis and a premise against the other text. This involves two checks:
# Forward Entailment: Whether the premise (first text) semantically entails the hypothesis (second text).
# Backward Entailment: Whether the hypothesis (second text) semantically entails the premise (first text).
# If both conditions are met, the texts are considered semantically equivalent, akin to paraphrases. Mathematically, this is often represented using probabilities derived from a model trained on entailment tasks, such as those derived from the MNLI dataset using a BERT model.

# Given probabilities of entailment (P) from a softmax layer for both forward and backward directions, the odds of entailment are calculated as:
# Odds = P/1-P ​
# The final score for bi-directional entailment could be the product of the odds for both directions, ensuring that high entailment probabilities in both directions yield a higher score
def check_entailment(premise, hypothesis):
    inputs = nli_tokenizer(premise, hypothesis, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = nli_model(**inputs)
    logits = outputs.logits
    entail_contradiction_logits = logits[:, [0, 2]]
    probs = F.softmax(entail_contradiction_logits, dim=1)
    entail_prob = probs[:, 0].item()
    return entail_prob

def cluster_answers(answers, question):
    answers = [answer[0] for answer in answers]
    clusters = []
    for answer in answers:
        added_to_cluster = False
        for cluster in clusters:
            representative_answer = cluster[0]
            forward_entail_prob = check_entailment(question + " " + representative_answer, answer)
            backward_entail_prob = check_entailment(question + " " + answer, representative_answer)
            if forward_entail_prob > 0.4 and backward_entail_prob > 0.4:
                cluster.append(answer)
                added_to_cluster = True
                break
        if not added_to_cluster:
            clusters.append([answer])
    return clusters

def calculate_average_clusters(all_question_data):
    total_correct_clusters = 0
    total_incorrect_clusters = 0
    num_correct_questions = 0
    num_incorrect_questions = 0
    for question_data in all_question_data:
        num_clusters = len(question_data["clusters"])
        is_correct = any(answer["correct"] for answer in question_data["generated_answers"])
        if is_correct:
            total_correct_clusters += num_clusters
            num_correct_questions += 1
        else:
            total_incorrect_clusters += num_clusters
            num_incorrect_questions += 1
    average_correct_clusters = total_correct_clusters / num_correct_questions if num_correct_questions > 0 else 0
    average_incorrect_clusters = total_incorrect_clusters / num_incorrect_questions if num_incorrect_questions > 0 else 0
    return average_correct_clusters, average_incorrect_clusters

def calculate_ptrue(question, generated_answers, top_n=5):
    # Format the prompt
    prompt = f"Question: {question}\nHere are some brainstormed ideas:\n"
    for answer, _ in generated_answers[:top_n]:
        prompt += answer + "\n"
    prompt += "Possible Answer: {}\nIs the possible answer: (A) True (B) False\nThe possible answer is:"

    inputs = qa_tokenizer(prompt, return_tensors="pt").to(device)
    # print(qa_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0]))

    with torch.no_grad():
        outputs = qa_model(**inputs)
        start_logits = outputs.start_logits[0]
        end_logits = outputs.end_logits[0]

        # Find the token index of "True" in the prompt
        true_token_id = qa_tokenizer.convert_tokens_to_ids("ĠTrue")
        true_token_indices = (inputs["input_ids"][0] == true_token_id).nonzero(as_tuple=True)[0]
        if true_token_indices.nelement() == 0:
          print(f"Warning: 'True' token not found in prompt for question: {question}")
          return 0.5
        else:
          true_token_index = true_token_indices[0].item()
        # Calculate the score for each answer span based on start/end logits
        answer_span_scores = []
        for start_idx, start_logit in enumerate(start_logits):
            for end_idx, end_logit in enumerate(end_logits[start_idx:]):
                real_end_idx = start_idx + end_idx
                if start_idx <= true_token_index <= real_end_idx:  # Check if span includes "True"
                    score = start_logit + end_logit
                    answer_span_scores.append(score)
                else:
                    answer_span_scores.append(torch.tensor(-float("inf")).to(device))  # Assign very low score

        # Find the answer span with the highest score
        scores_tensor = torch.tensor(answer_span_scores)
        scores_softmax = F.softmax(scores_tensor, dim=0)
        p_true = scores_softmax[torch.argmax(scores_tensor)].item()

    return p_true
def calculate_lexical_similarity(answers, reference_answers):
    if not reference_answers:
        return 0.5
    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
    scores = []
    for answer in answers:
        max_score = max(scorer.score(answer, ref)['rouge1'].fmeasure for ref in reference_answers)
        scores.append(max_score)
    average_score = sum(scores) / len(scores) if scores else 0.5
    return average_score

def calculate_semantic_entropy(cluster_probabilities):
    entropy = 0
    for prob in cluster_probabilities:
        if prob > 0:
            entropy -= prob * math.log2(prob)
    return entropy

def calculate_seq_log_prob(question, context, answer, model, tokenizer):
    input_text = question + ' ' + context
    output_text = answer

    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
    output_ids = tokenizer.encode(output_text, return_tensors='pt').to(device)

    log_prob_sum = 0

    for i in range(1, len(output_ids[0])):
        previous_tokens = output_ids[:, :i]
        target_token = output_ids[:, i]

        with torch.no_grad():
            outputs = model(previous_tokens)
            logits = outputs.logits[:, i-1, :]
            log_probs = F.log_softmax(logits, dim=-1)
            log_prob = log_probs[0, target_token].item()

        log_prob_sum += log_prob

    seq_log_prob = log_prob_sum / len(output_ids[0])
    return seq_log_prob


def calculate_mrr(correct_labels, scores):
    sorted_indices = np.argsort(-np.array(scores))
    sorted_labels = np.array(correct_labels)[sorted_indices]
    ranks = np.where(sorted_labels == 1)[0] + 1
    mrr = np.sum(1 / ranks) / len(correct_labels)
    return mrr

def calculate_cws(correct_labels, scores):
    sorted_indices = np.argsort(-np.array(scores))
    sorted_labels = np.array(correct_labels)[sorted_indices]
    cws = np.sum(sorted_labels * np.arange(1, len(scores) + 1)) / np.sum(np.arange(1, len(scores) + 1))
    return cws

def calculate_k_measure(correct_labels, scores, k=10):
    sorted_indices = np.argsort(-np.array(scores))[:k]
    sorted_labels = np.array(correct_labels)[sorted_indices]
    k_measure = np.sum(sorted_labels) / k
    return k_measure

def calculate_rote_metric(question, context, answer, qa_model, qa_tokenizer):
    """
    Calculate the "rote metric" for a given question, context, and answer.

    Args:
        question (str): The question.
        context (str): The context.
        answer (str): The generated answer.
        qa_model (transformers.AutoModelForQuestionAnswering): The question answering model.
        qa_tokenizer (transformers.AutoTokenizer): The tokenizer for the model.

    Returns:
        float: The rote metric score.
    """

    # Encode the question, context, and answer
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)

    # Get model outputs with hidden states
    with torch.no_grad():
        outputs = qa_model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1][0]  # Get last layer hidden states

    # Calculate question contribution
    question_tokens = qa_tokenizer(question, return_tensors="pt")["input_ids"][0]
    question_length = len(question_tokens)
    question_contribution = torch.sum(hidden_states[:question_length], dim=0)

    # Calculate answer contribution
    answer_tokens = qa_tokenizer(answer, return_tensors="pt")["input_ids"][0]
    answer_length = len(answer_tokens)
    answer_hidden = qa_model.get_input_embeddings()(answer_tokens.to(device))
    answer_contribution = torch.sum(answer_hidden, dim=0)

    # Calculate similarity score
    similarity = torch.cosine_similarity(question_contribution.unsqueeze(0),
                                      answer_contribution.unsqueeze(0),
                                      dim=1)

    return similarity.item()

def calculate_learn_metric(question, context, answer, qa_model, qa_tokenizer):
    """
    Calculate the "learn metric" for a given question, context, and answer.

    Args:
        question (str): The question.
        context (str): The context.
        answer (str): The generated answer.
        qa_model (transformers.AutoModelForQuestionAnswering): The question answering model.
        qa_tokenizer (transformers.AutoTokenizer): The tokenizer for the model.

    Returns:
        float: The learn metric score.
    """

    # Encode inputs
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)
    answer_inputs = qa_tokenizer(answer, return_tensors="pt").to(device)

    with torch.no_grad():
        # Get hidden states for question+context
        outputs = qa_model(**inputs, output_hidden_states=True)
        context_states = outputs.hidden_states[-1][0]

        # Get hidden states for answer
        answer_outputs = qa_model(**answer_inputs, output_hidden_states=True)
        answer_states = answer_outputs.hidden_states[-1][0]

        # Calculate mean embeddings
        context_embed = context_states.mean(dim=0)
        answer_embed = answer_states.mean(dim=0)

        # Calculate similarity
        similarity = torch.cosine_similarity(
            context_embed.unsqueeze(0),
            answer_embed.unsqueeze(0)
        )

        return similarity.item()

def calculate_similarity_metric(question, context, answer, qa_model, qa_tokenizer):
    """
    Calculate the cosine similarity between the answer and the generated answer.

    Args:
        question (str): The question.
        context (str): The context.
        answer (str): The reference answer.
        qa_model (transformers.AutoModelForQuestionAnswering): The question answering model.
        qa_tokenizer (transformers.AutoTokenizer): The tokenizer for the model.

    Returns:
        float: The cosine similarity score.
    """

    # Encode inputs
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)
    answer_inputs = qa_tokenizer(answer, return_tensors="pt").to(device)

    with torch.no_grad():
        # Get hidden states
        outputs = qa_model(**inputs, output_hidden_states=True)
        question_context_states = outputs.hidden_states[-1][0]

        answer_outputs = qa_model(**answer_inputs, output_hidden_states=True)
        answer_states = answer_outputs.hidden_states[-1][0]

        # Calculate embeddings
        qc_embed = question_context_states.mean(dim=0)
        answer_embed = answer_states.mean(dim=0)

        # Calculate similarity
        similarity = torch.cosine_similarity(
            qc_embed.unsqueeze(0),
            answer_embed.unsqueeze(0)
        )

        return similarity.item()

def calculate_distance_metric(question, context, answer, qa_model, qa_tokenizer):
    """
    Calculate the Euclidean distance between the answer and the generated answer.

    Args:
        question (str): The question.
        context (str): The context.
        answer (str): The reference answer.
        qa_model (transformers.AutoModelForQuestionAnswering): The question answering model.
        qa_tokenizer (transformers.AutoTokenizer): The tokenizer for the model.

    Returns:
        float: The Euclidean distance score.
    """

    # Encode inputs
    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors="pt").to(device)
    answer_inputs = qa_tokenizer(answer, return_tensors="pt").to(device)

    with torch.no_grad():
        # Get hidden states
        outputs = qa_model(**inputs, output_hidden_states=True)
        qc_states = outputs.hidden_states[-1][0]

        answer_outputs = qa_model(**answer_inputs, output_hidden_states=True)
        answer_states = answer_outputs.hidden_states[-1][0]

        # Calculate mean embeddings
        qc_embed = qc_states.mean(dim=0)
        answer_embed = answer_states.mean(dim=0)

        # Calculate Euclidean distance
        distance = torch.norm(qc_embed - answer_embed)

        # Normalize to [0,1] range
        normalized_distance = 1 - torch.exp(-distance/10).item()

        return normalized_distance

type(train_data)

len(train_data)

subset_size = int(len(train_data) * 0.1)

# Select the first 10% of the dataset
subset_data = train_data.select(range(subset_size))

len(subset_data)

selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)
selfcheck_ngram = SelfCheckNgram(n=3)
selfcheck_nli = SelfCheckNLI(device=device)

all_question_data = []
correct_output = []
max_iterations = 1
all_context_data=[]
auroc_ptrue =0
auroc_lexical =0
auroc_entropy =0.15
auroc_rote = 0
auroc_learn = 0
auroc_similarity = 0
auroc_distance = 0
auroc_selfcheck_bertscore = 0
auroc_selfcheck_ngram = 0
auroc_selfcheck_nli = 0
x=0

from tqdm import tqdm
for context_idx, coqa_example in tqdm(enumerate(subset_data), total=len(subset_data), desc="Processing CoQA Examples"):
    # if x>= max_iterations:
    #     break
    # x+=1
    context_data = {"context_index": context_idx, "questions": []}
    print("PARA ITERATION")
    # k = 0

    # k += 1
    # if k % 10 == 0:
    #     print(k)
    context = coqa_example['story']
    # print(context)

    n_qa = len(coqa_example['questions'])
    question_list = coqa_example['questions']
    answers_list = coqa_example['answers']['input_text']

    for n in range(n_qa):
        question = question_list[n]
        reference_answers = [answers_list[n]]
        question_data = {"question": question, "reference_answers": reference_answers, "generated_answers": []}

        # Generate answers using the model (on GPU)
        model_answers_with_confidence = generate_answers(question, context, top_n=5)

        # Evaluate and store correct/incorrect labels
        for answer, confidence in model_answers_with_confidence:
            if not reference_answers:
                # print(f"Skipping question with no reference answers: {question}")
                continue
            rouge_score = rouge_metric.compute(predictions=[answer], references=reference_answers, rouge_types=["rougeL"])
            rouge_l_score = rouge_score["rougeL"]
            correct_output.append(1 if rouge_l_score > 0.3 else 0)
            question_data["generated_answers"].append({"answer": answer, "confidence": confidence, "rougeL_score": rouge_l_score, "correct": correct_output[-1]})

        # Calculate p(True) and store it in question_data

        # Calculate lexical similarity and store it in question_data
        if reference_answers:
            question_data["lexical_similarity"] = calculate_lexical_similarity([answer[0] for answer in model_answers_with_confidence], reference_answers)
            question_data["p_true"] = calculate_ptrue(question, model_answers_with_confidence, top_n=5)
            clusters = cluster_answers(model_answers_with_confidence, question)
            question_data["clusters"] = clusters
            context_data["questions"].append(question_data)

    all_context_data.append(context_data)

# Calculate average clusters for all questions
average_correct_clusters, average_incorrect_clusters = calculate_average_clusters([q for c in all_context_data for q in c["questions"]])

# Add average cluster information to each context
for context_data in all_context_data:
    context_data["average_correct_clusters"] = average_correct_clusters
    context_data["average_incorrect_clusters"] = average_incorrect_clusters

# Store data in a JSON file
with open("context_question_clustering_data.json", "w") as f:
    json.dump(all_context_data, f, indent=4)

import warnings
from transformers import logging as hf_logging

# Suppress specific FutureWarning from the transformers library
warnings.filterwarnings("ignore", category=FutureWarning, message=".*clean_up_tokenization_spaces.*")

# Optionally, you can suppress other transformers-related warnings if needed
hf_logging.set_verbosity_error()

# Your existing code...

# Initialize the selfcheck models with corrected configurations
selfcheck_bertscore = SelfCheckBERTScore(
    rescale_with_baseline=True
)

selfcheck_nli = SelfCheckNLI(device=device)  # Move to device after initialization
# Iterate over the data and calculate sequence log probabilities
with open("/kaggle/working/context_question_clustering_data.json", "r") as f:
    all_context_data = json.load(f)
for context_data in tqdm(all_context_data, desc="Processing Context Data", unit="context"):
    context_index = context_data["context_index"]
    context = subset_data[context_index]["story"]
    for question_data in context_data["questions"]:
        question = question_data["question"]
        reference_answers = question_data["reference_answers"]
        generated_answers = question_data["generated_answers"]
        log_probs = []
        for answer_dict in generated_answers:
            answer = answer_dict["answer"]
            log_prob = calculate_seq_log_prob(question, context, answer, clm_model, clm_tokenizer)
            log_probs.append(log_prob)
            answer_dict["log_prob"] = log_prob
            answer_dict["rote_metric"] = calculate_rote_metric(question, context, answer, qa_model, qa_tokenizer)
            answer_dict["learn_metric"] = calculate_learn_metric(question, context, answer, qa_model, qa_tokenizer)
            answer_dict["similarity_metric"] = calculate_similarity_metric(question, context, answer, qa_model, qa_tokenizer)
            answer_dict["distance_metric"] = calculate_distance_metric(question, context, answer, qa_model, qa_tokenizer)
            answer_dict["selfcheck_bertscore"] = selfcheck_bertscore.predict(
                sentences=[answer],
                sampled_passages=[context],
            )[0]
            answer_dict["selfcheck_nli"] = selfcheck_nli.predict(
                sentences=[answer],
                sampled_passages=[context],
            )[0]
        avg_log_prob = sum(log_probs) / len(log_probs)
        question_data["avg_log_prob"] = avg_log_prob
with open("/kaggle/working/context_question_clustering_data.json", "w") as f:
    json.dump(all_context_data, f, indent=4)

all_question_entropy_data=[]
# Calculate semantic entropy for each question and store in a new JSON fileall_question_entropy_data = []
with open("context_question_clustering_data.json", "r") as f:
    all_context_data = json.load(f)
for context_data in all_context_data:
    for question_data in context_data["questions"]:
        clusters = question_data["clusters"]
        answer_confidences = {answer["answer"]: answer["confidence"] for answer in question_data["generated_answers"]}
        total_prob=sum(sum(answer_confidences[answer] for answer in cluster) for cluster in clusters)
        cluster_probabilities = [
            sum(answer_confidences[answer] for answer in cluster)/total_prob for cluster in clusters
        ]
        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)
        question_data["semantic_entropy"] = semantic_entropy
        all_question_entropy_data.append(question_data)
with open("question_entropy_data.json", "w") as f:
    json.dump(all_question_entropy_data, f, indent=4)

ptrue_values = []
lexical_similarity_values = []
entropy_values = []
correct_labels = []
rote_values = []
learn_values = []
similarity_values = []
distance_values = []
selfcheck_bertscore_values = []
selfcheck_nli_values = []
for question_data in all_question_entropy_data:
    for answer in question_data["generated_answers"]:
        ptrue_values.append(question_data["p_true"])
        lexical_similarity_values.append(question_data["lexical_similarity"])
        entropy_values.append(question_data["semantic_entropy"])
        rote_values.append(answer["rote_metric"])
        learn_values.append(answer["learn_metric"])
        similarity_values.append(answer["similarity_metric"])
        distance_values.append(answer["distance_metric"])
        selfcheck_bertscore_values.append(answer["selfcheck_bertscore"])
        selfcheck_nli_values.append(answer["selfcheck_nli"])
        correct_labels.append(answer["correct"])

# Calculate AUROC scores
auroc_ptrue += roc_auc_score(correct_labels, ptrue_values)
auroc_lexical += roc_auc_score(correct_labels, lexical_similarity_values)
auroc_entropy += roc_auc_score(correct_labels, entropy_values)
auroc_rote += roc_auc_score(correct_labels, rote_values)
auroc_learn += roc_auc_score(correct_labels, learn_values)
auroc_similarity += roc_auc_score(correct_labels, similarity_values)
auroc_distance += roc_auc_score(correct_labels, distance_values)
auroc_selfcheck_bertscore += roc_auc_score(correct_labels, selfcheck_bertscore_values)
auroc_selfcheck_nli += roc_auc_score(correct_labels, selfcheck_nli_values)

# Store AUROC scores in a dictionary
auroc_scores = {
    "p_true": auroc_ptrue,
    "lexical_similarity": auroc_lexical,
    "semantic_entropy": auroc_entropy,
    "rote_metric": auroc_rote,
    "learn_metric": auroc_learn,
    "similarity_metric": auroc_similarity,
    "distance_metric": auroc_distance,
    "selfcheck_bertscore": auroc_selfcheck_bertscore,
    "selfcheck_nli": auroc_selfcheck_nli
}
from sklearn.metrics import precision_recall_fscore_support

# Calculate and print precision, recall, and f1-score
precision, recall, f1, _ = precision_recall_fscore_support(correct_labels, [1 if score > 0.5 else 0 for score in ptrue_values], average='binary')
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Calculate and print Mean Reciprocal Rank (MRR)
mrr_ptrue = calculate_mrr(correct_labels, ptrue_values)
mrr_lexical = calculate_mrr(correct_labels, lexical_similarity_values)
mrr_entropy = calculate_mrr(correct_labels, entropy_values)
mrr_rote = calculate_mrr(correct_labels, rote_values)
mrr_learn = calculate_mrr(correct_labels, learn_values)
mrr_similarity = calculate_mrr(correct_labels, similarity_values)
mrr_distance = calculate_mrr(correct_labels, distance_values)
mrr_selfcheck_bertscore = calculate_mrr(correct_labels, selfcheck_bertscore_values)
mrr_selfcheck_nli = calculate_mrr(correct_labels, selfcheck_nli_values)
print(f"MRR (p_true): {mrr_ptrue:.4f}")
print(f"MRR (lexical_similarity): {mrr_lexical:.4f}")
print(f"MRR (semantic_entropy): {mrr_entropy:.4f}")
print(f"MRR (rote_metric): {mrr_rote:.4f}")
print(f"MRR (learn_metric): {mrr_learn:.4f}")
print(f"MRR (similarity_metric): {mrr_similarity:.4f}")
print(f"MRR (distance_metric): {mrr_distance:.4f}")
print(f"MRR (selfcheck_bertscore): {mrr_selfcheck_bertscore:.4f}")
print(f"MRR (selfcheck_nli): {mrr_selfcheck_nli:.4f}")

# Calculate and print Confidence Weighted Score (CWS)
cws_ptrue = calculate_cws(correct_labels, ptrue_values)
cws_lexical = calculate_cws(correct_labels, lexical_similarity_values)
cws_entropy = calculate_cws(correct_labels, entropy_values)
cws_rote = calculate_cws(correct_labels, rote_values)
cws_learn = calculate_cws(correct_labels, learn_values)
cws_similarity = calculate_cws(correct_labels, similarity_values)
cws_distance = calculate_cws(correct_labels, distance_values)
cws_selfcheck_bertscore = calculate_cws(correct_labels, selfcheck_bertscore_values)
cws_selfcheck_nli = calculate_cws(correct_labels, selfcheck_nli_values)
print(f"CWS (p_true): {cws_ptrue:.4f}")
print(f"CWS (lexical_similarity): {cws_lexical:.4f}")
print(f"CWS (semantic_entropy): {cws_entropy:.4f}")
print(f"CWS (rote_metric): {cws_rote:.4f}")
print(f"CWS (learn_metric): {cws_learn:.4f}")
print(f"CWS (similarity_metric): {cws_similarity:.4f}")
print(f"CWS (distance_metric): {cws_distance:.4f}")
print(f"CWS (selfcheck_bertscore): {cws_selfcheck_bertscore:.4f}")
print(f"CWS (selfcheck_nli): {cws_selfcheck_nli:.4f}")

# Calculate and print K-Measure
k = 10
k_measure_ptrue = calculate_k_measure(correct_labels, ptrue_values, k)
k_measure_lexical = calculate_k_measure(correct_labels, lexical_similarity_values, k)
k_measure_entropy = calculate_k_measure(correct_labels, entropy_values, k)
k_measure_rote = calculate_k_measure(correct_labels, rote_values, k)
k_measure_learn = calculate_k_measure(correct_labels, learn_values, k)
k_measure_similarity = calculate_k_measure(correct_labels, similarity_values, k)
k_measure_distance = calculate_k_measure(correct_labels, distance_values, k)
k_measure_selfcheck_bertscore = calculate_k_measure(correct_labels, selfcheck_bertscore_values, k)
k_measure_selfcheck_nli = calculate_k_measure(correct_labels, selfcheck_nli_values, k)
print(f"K-Measure (p_true, k={k}): {k_measure_ptrue:.4f}")
print(f"K-Measure (lexical_similarity, k={k}): {k_measure_lexical:.4f}")
print(f"K-Measure (semantic_entropy, k={k}): {k_measure_entropy:.4f}")
print(f"K-Measure (rote_metric, k={k}): {k_measure_rote:.4f}")
print(f"K-Measure (learn_metric, k={k}): {k_measure_learn:.4f}")
print(f"K-Measure (similarity_metric, k={k}): {k_measure_similarity:.4f}")
print(f"K-Measure (distance_metric, k={k}): {k_measure_distance:.4f}")
print(f"K-Measure (selfcheck_bertscore, k={k}): {k_measure_selfcheck_bertscore:.4f}")
print(f"K-Measure (selfcheck_nli, k={k}): {k_measure_selfcheck_nli:.4f}")

# Save AUROC scores to a JSON file
with open("auroc_scores.json", "w") as f:
    json.dump(auroc_scores, f, indent=4)

# Print AUROC scores
print("AUROC Scores:")
for metric, score in auroc_scores.items():
    print(f"{metric}: {score:.4f}")